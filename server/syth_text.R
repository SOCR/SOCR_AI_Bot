# load demo data when clicked
observeEvent(input$demo_synth_prompt,{
  if(input$demo_synth_prompt != demo_synth_prompts[1]) {
    updateTextInput(
      session,
      "text_prompt",
      value = input$demo_synth_prompt
    )
  } else { # if not mpg data, reset
    updateTextInput(
      session,
      "text_prompt",
      value = "",
      placeholder = "Provide a prompt for synthetic text generation, see examples. 
          Audio commands are currently disabled, they can be reenabled by using the 
          \'heyshiny\' package and by specifying voice narration in \'Settings\'."
    )
  }
})

output$synth_text <- renderText({
  req(input$prompt_button)
  
  isolate({
    req(input$text_prompt) 
    
    #----------------------------Prep question
    txt <- input$text_prompt
    
    # force to within 280 characters
    if (nchar(txt) > max_char_question) {
      txt <- substr(txt, 1, max_char_question)
      showNotification(
        paste("Only the first", max_char_question, " characters will be used."),
        duration = 10
      )
    }
    
    # If the last character is not a stop, add it.
    # Otherwise, GPT3 will add a sentence.
    
    # The following 5 lines were generated by ChatGPT!!!!!
    # Check if the last character is not a period
    if (substr(txt, nchar(txt), nchar(txt)) != ".") {
      # If the last character is not a period, add it to the end
      txt <- paste(txt, ".", sep = "")
    }
    
    prepared_request <- paste(
      "If the question is not related to statistics or data science 
        say 'Statistics only!' ",
      txt
    )
    prepared_request <- txt
    
    #----------------------------Send request
    shinybusy::show_modal_spinner(
      spin = "semipolar",
      text = paste0("... Please wait, processing ...\n\n",
                    prepared_request # sample(jokes, 1)
      ),
      color = "#000000"
    )
    
    start_time <- Sys.time()
    
    # Send to openAI
    # create_completion(model = "text-davinci-003", n=5, best_of=10,
    #                   max_tokens=300, temperature=0.8, # 
    #                   prompt = "Write an abstract of a healthcare essay")
    
    
    #fix this here again
    
    chat <- function(input_message){
      user_input <- list(list(role = "user", content = input_message))
      base_url <- "https://api.openai.com/v1/chat/completions"
      api_key_openAI <- api_key_session()$api_key_openAI
      body <- list(model = language_model,
                   messages = user_input,
                   max_tokens = 200,
                   temperature = sample_temp())
      req <- request(base_url)
      resp <-
        req |>
        req_auth_bearer_token(token = api_key_openAI) |>
        req_headers("Content-Type" = "application/json") |>
        req_body_json(body) |>
        req_retry(max_tries = 4) |>
        req_throttle(rate = 15) |>
        req_perform()
      
      openai_chat_response <- resp |> resp_body_json(simplifyVector = TRUE)
      #print(openai_chat_response) #shows more details of the result
      #print(openai_chat_response$choices$message$content)
      return(openai_chat_response)
    }
    #model = language_model,
    #prompt = prepared_request,
    #openai_api_key = api_key_session()$api_key,
    #max_tokens = 200,
    #temperature = sample_temp()
    
    tryCatch(
      response <- chat(prepared_request),
      error = function(e) {
        # remove spinner, show message for 5s, & reload
        shinybusy::remove_modal_spinner()
        shiny::showModal(api_error_modal)
        Sys.sleep(5)
        session$reload()
        
        list(
          error_value = -1,
          message = capture.output(print(e$message)),
          error_status = TRUE
        )
      }
    )
    
    error_api <- FALSE
    # if error returns true, otherwise 
    #  that slot does not exist, returning false.
    # or be NULL
    error_api <- tryCatch(
      !is.null(response$error_status),
      error = function(e) {
        return(TRUE)
      }
    )
    
    error_message <- NULL
    if (error_api) {
      cmd <- NULL
      response <- NULL
      error_message <- response$message
    } else {
      cmd <- response$choices$message$content
    }
    
    api_time <- difftime(
      Sys.time(),
      start_time,
      units = "secs"
    )[[1]]
    
    # if more than 10 requests, slow down. Only on the server.
    if (counter$requests > 20 && file.exists(on_server)) {
      Sys.sleep(counter$requests / 5 + runif(1, 0, 5))
    }
    if (counter$requests > 50 && file.exists(on_server)) {
      Sys.sleep(counter$requests / 10 + runif(1, 0, 10))
    }
    if (counter$requests > 100 && file.exists(on_server)) {
      Sys.sleep(counter$requests / 40 + runif(1, 0, 40))
    }
    
    shinybusy::remove_modal_spinner()
    
    # update usage via global reactive value
    counter$tokens <- counter$tokens + response$usage$completion_tokens
    counter$requests <- counter$requests + 1
    counter$time <- round(api_time, 0)
    counter$tokens_current <- response$usage$completion_tokens
    
    humor <- c(
      "Seriously? Statistics only!",
      "Come on. Statistics only!",
      "You know better. Statistics only!",
      "Bruh... I am a statistics tutor! ",
      "Are you kidding? Statistics only!",
      "Gee..., Statistics only!!"
    )
    
    
    ans <- response$choices$message$content   #ans <- response$choices[1, 1]
    #if (grepl("Statistics only!", ans)) {
    #  ans <- paste(
    #    sample(humor, 1),
    #    "     If you are not
    # trying to be funny, ask again with more context. It might
    #  be helpful to add \"in statistics\" to the question."
    #  )
    #}
    return(ans)
  })
  
})